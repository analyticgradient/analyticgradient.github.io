<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Oracle attacks on convnets</title>
  <meta name="description" content="The dream: pwn a convolutional neural network. The goal: teach myself something. The disclaimer: I’m not a deep learning expert.">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://analyticgradient.com/oracle-attacks-on-convnets.html">
  <link rel="alternate" type="application/rss+xml" title="Analytic Gradient" href="http://analyticgradient.com/feed.xml">

  <link href="https://fonts.googleapis.com/css?family=Space+Mono" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Noto+Serif" rel="stylesheet">
  <link href="/css/syntax.css" rel="stylesheet">
</head>


  <body>

    <header id="site-header">
  <h1><a href="/">ANALYTIC GRADIENT</a></h1>

    <p id="quote_display">
      the revolution will be supervised
    </p>

    <p>
      a blog about economics, computer science, and machine learning
    </p>

    <p>
      <strong>Established 2017. Melbourne &amp; Sydney, Australia</strong>
    </p>
</header>


    <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="page-heading">Oracle attacks on convnets</h1>
    <p class="post-meta"><time datetime="2017-07-25T00:00:00+10:00" itemprop="datePublished">25 July 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>The dream: pwn a convolutional neural network. The goal: teach myself something. The disclaimer: I’m not a deep learning expert.</p>

<h3 id="deep-learning-is-everywhere">Deep learning is everywhere</h3>
<p>Deep learning is the hot new thing. Well, it’s not really new anymore. State-of-the-art?</p>

<p>If it’s not already everywhere it soon will be. Go champions have been defeated. Self-driving cars are recognising things on the road thanks to deep neural networks (DNNs). Presumably, surveillance of all stripes is being upgraded with DNNs. Malware and network intrusions are being detected. And evaded.</p>

<p>However, kind of like the internet, machine learning was never built to be secure.</p>

<h3 id="adversarial-machine-learning">Adversarial machine learning</h3>
<p>We’re sitting in your self-driving car as it zooms down a busy road. Halfway through the commute, the cameras in our car see a stop sign. Instead of stopping, our car accelerates through an intersection. We die. Or at least, we’re severely injured. I wonder how insurance works with self-driving cars?</p>

<p>What just happened? Did a hacker just take over our car? Not directly in this case – our car <strong>misinterpreted the stop sign as something else</strong>. The hacker caused our car to misinterpret its environment. Maybe they used a marker to add a few dots to the sign.</p>

<p>Standard machine learning algorithms aren’t built to handle an adversary. They ingest their training data assuming nothing is wrong. They predict labels for new data, also assuming nothing is wrong. If we imagine ourselves the hacker, we can start to see a few weak points. There’s a good discussion on attacking ML algorithms in <a href="http://dl.acm.org/citation.cfm?id=2046692">Huang et al. (2011)</a>. Let’s begin with a quick summary.</p>

<h3 id="a-taxonomy-of-attacks">A taxonomy of attacks</h3>
<p>Consider first at what stage in the ML pipeline we target our attacks. Attacks can be <strong>causative</strong>, where we target the training data. If we manage to poison this, the classifier will infer a poor model. We could potentially control what kind of poor that classifier is – but either way, we can exploit it. Attacks can also be <strong>exploratory</strong> – we’ll probe the model after it’s been trained, trying to find weak spots. Maybe we learn something about how it works, or what it was trained on. Maybe we can force it to misclassify something.</p>

<p>Attacks can also vary on how “large” they are. <strong>Integrity</strong> attacks aim for false negatives – we can slip something by the classifier undetected. <strong>Availability</strong> attacks aim to cause so many missclassifications – whether false positives or negatives – that the classifier essentially becomes unusable. It’s a bit like a denial of service attack.</p>

<p>Finally, attacks can be <strong>targeted</strong> or <strong>indiscriminate</strong>.</p>

<h3 id="lets-attack-a-classifier">Let’s attack a classifier</h3>
<p>Adversarial machine learning sounded really cool. I figured the best way to learn something was to actually attack some classifier. With that in mind, I’ll be implementing a black-box attack as described in <a href="https://arxiv.org/abs/1602.02697">Papernot et al. (2016)</a> using a technique from <a href="https://arxiv.org/abs/1412.6572">Goodfellow et al. (2015)</a>. In the above taxonomy, this is an <strong>explorative</strong> attack.</p>

<p>I’ll be using python and tensorflow for this, targeting a convolutional neural network trained on <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> handwritten digit data per the <a href="https://www.tensorflow.org/get_started/mnist/pros">tensorflow tutorial</a>. Here’s some <a href="https://www.youtube.com/watch?v=uoZgZT4DGSY">music (nsfw?)</a> I used to get into character.</p>

<p>Just before we begin, note that there is a python library called <a href="https://github.com/tensorflow/cleverhans"><strong>cleverhans</strong></a> that implements this attack. In fact, the authors of the paper we’re implementing contributed. What follows is half me reading the paper(s) and half pulling the library apart in an attempt to understand the attack.</p>

<h3 id="anatomy-of-an-attack">Anatomy of an attack</h3>
<p>Most attacks tend to assume you have access to the target classifier internals (parameters, gradients, et al.). Then, given this information, you can craft <strong>adversarial samples</strong> that cause it to missclassify (here’s a <a href="https://blog.openai.com/robust-adversarial-inputs/">cute example involving a kitten</a>).</p>

<p>The Papernot et al. (2016) black-box attack (henceforth just black-box attack) assumes that we don’t have aceess to this information. We only have access to the predictions – that is, we have access to the target as an <em>oracle</em>. We can query this oracle with some data and it will give us a prediction.</p>

<p>The black-box attack works by exploiting this access to train a substitute model. This substitute model essentially <strong>duplicates</strong> the target model. We’re not out to maximise accuracy directly, but to learn the target’s decision boundaries.</p>

<p>In fact, we don’t even have to use the same model as the target. This is handy, because we may not know what it is. We could make some educated guesses, but this makes the attack more realistic. In this example, the target model is a convolutional neural network and the adversarial model is plain old multinomial logit.</p>

<p>To train the adversarial model, we start with 100 samples from the MNIST training set. We then get the oracle to labels these for us. That defines our training labels. Our training loop then looks like this:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">adv_train_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">adv_train_set</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">adv_train_epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">adv_train_epochs</span><span class="p">):</span>
    <span class="n">oracle_labels</span> <span class="o">=</span> <span class="n">oracle_predict</span><span class="p">(</span><span class="n">adv_train_set</span><span class="p">)</span>

    <span class="n">train</span><span class="p">(</span><span class="n">adv_model</span><span class="p">,</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">adv_train_set</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">oracle_labels</span><span class="p">})</span>

    <span class="n">adv_train_set</span> <span class="o">=</span> <span class="n">augment</span><span class="p">(</span><span class="n">adv_train_set</span><span class="p">)</span></code></pre></figure>

<p>Here, we run through <code class="highlighter-rouge">adv_train_epochs</code> of adversarial training. Each epoch is called a substitute training epoch (although I’ve called it adversarial training epoch in the code). The only other thing that requires explanation here is the <code class="highlighter-rouge">augment</code> step. What it does is build a new synthetic dataset based on the old one. It takes every training sample we have, perturbs it a bit (<code class="highlighter-rouge">new_example = example + lambda*pertubation_vector</code> where lambda is a pertubation factor). This new, perturbed dataset is then added to the old one. So if we had 100 samples, now we have 200, where half of them are perturbed versions of the other half.</p>

<p>But we don’t just use white noise to generate the pertubation vector. We use a heuristic called Jacobian-based dataset augmentation. First, we grab the <a href="http://mathworld.wolfram.com/Jacobian.html">Jacobian</a> of our adversarial model (a matrix of gradients of our predictions – of the adversarial model – with respect to inputs). We then evaluate it at every training sample we have. Finally, we take the dimension of that matrix depending on how the oracle labels that particular sample. So in practice the Jacobian is a list of lists – 10 lists of 784 elements (28x28 MNIST images). If the oracle predicts a 7, we choose the 7th out of 10. Therefore, we can define some helper functions:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
    <span class="c">#That is, how does the kth element of yhat vary wrt x?</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">predictions</span><span class="p">[:,</span> <span class="n">c</span><span class="p">],</span> <span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">jacobian_prediction_dimension</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">grads</span><span class="p">[</span><span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">))]</span></code></pre></figure>

<p>This information – how our model ourput varies with respect to inputs – lets us generate some <strong>useful variance</strong> from which to learn the oracle’s decision boundaries. To complete the heuristic, we take the sign of our selected dimension of the Jacobian and then add it to the original example subject to some pertubation factor lambda. Lambda can vary over each training epoch, here we’ve made it flip sign every <code class="highlighter-rouge">tau</code> epochs. The code for the <code class="highlighter-rouge">augment</code> heuristic:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#Jacobian-based dataset augmentation</span>
<span class="c">#note that yhat is the logit output of the adversary</span>
<span class="c">#and oracle_labels is one-hot encoded</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">jacobian</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">xm</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xm</span><span class="p">:</span> <span class="n">adv_train_set</span><span class="p">})</span>
<span class="n">jpd</span> <span class="o">=</span> <span class="n">jacobian_prediction_dimension</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">oracle_labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">perturbed_set</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">jbda_epoch_lambda</span> <span class="o">=</span> <span class="n">jbda_lambda</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">adv_train_epoch</span><span class="o">/</span><span class="n">tau</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">example</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">adv_train_set</span><span class="p">):</span>
    <span class="n">new_example</span> <span class="o">=</span> <span class="n">example</span> <span class="o">+</span> <span class="n">jbda_epoch_lambda</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">jpd</span><span class="p">[</span><span class="n">idx</span><span class="p">]))</span>
    <span class="n">perturbed_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_example</span><span class="p">)</span>
<span class="n">adv_train_set</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">adv_train_set</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">perturbed_set</span><span class="p">)))</span></code></pre></figure>

<p>And that’s it! We’ve now trained the adversary.</p>

<h3 id="generating-adversarial-examples">Generating adversarial examples</h3>
<p>Now we want to generate some adversarial examples that pass the human sniff test but are missclassified.</p>

<p>Now that we have a substitute model we can use some of the white-box attacks developed elsewhere in the literature. Recall that these attacks assumed we have some knowledge of the target classifier. Now we do – we’ve trained a substitute model to mimic the target’s decision boundaries!</p>

<p>Generating adversarial examples involves perturbing some original input in some fashion. Right off the bat we can think of one method – what if we just added random pertubations? A smarter idea might be to run some sort of optimisation method over the noise we generate – genetic algorithms, particle swarm optimisation, simulated annealing. This is feasible since we have access to a substitute model (which we can query in our own time without being detected). But it may not be the smartest.</p>

<p>I use the method described in Goodfellow et al. (2015) – henceforth Goodfellow attack or Fast Gradient Sign Method. Essentially, this attack perturbs all of the image a little bit (there’s another attack that perturbs some of the image a lot described in the paper). We calculate the pertubation vector in a similar fashion to how we calculate it for Jacobian-based dataset augmentation above. Except the gradient vector is of the loss function with respect to inputs, given model outputs (to prevent label leakage, described in <a href="https://arxiv.org/abs/1611.01236">Kurakin et al. 2016</a> – thanks cleverhans!)</p>

<p>Here’s how I did it:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#Goodfellow attack (FGSA)</span>
<span class="n">goodfellow_eps</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="c">#grab the adversary predictions of the test data to use as labels: adv_onehot</span>
<span class="c">#define loss function</span>
<span class="n">goodfellow_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">adv_onehot</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">yhat</span><span class="p">))</span>

<span class="c">#calculate the signed gradient matrix of loss wrt x given labels</span>
<span class="n">fgsa</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">goodfellow_loss</span><span class="p">,</span> <span class="n">xm</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

<span class="c">#add it to the examples subject to a pertubation factor epsilon</span>
<span class="n">adv_test</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">xm</span> <span class="o">+</span> <span class="n">goodfellow_eps</span> <span class="o">*</span> <span class="n">fgsa</span><span class="p">)</span>

<span class="c">#clip to remain in the MNIST domain [0, 1]</span>
<span class="n">adv_test_clip</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">adv_test</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="c">#get tensorflow to generate them</span>
<span class="n">adv_examples</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">adv_test_clip</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xm</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">})</span></code></pre></figure>

<p>And then some reports:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#calculate accuracy of oracle on normal test data and perturbed test data</span>
<span class="n">test_acc_nonadv</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span> <span class="o">=</span>
  <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
<span class="n">test_acc_adv</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span> <span class="o">=</span>
  <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">adv_examples</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Test accuracy </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">test_acc_nonadv</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test accuracy (after attack) </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">test_acc_adv</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy reduction </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_acc_nonadv</span> <span class="o">-</span> <span class="n">test_acc_adv</span><span class="p">))</span></code></pre></figure>

<p>After running it a few times (rather quickly, because logits are fast to train), I usually got an accuracy reduction of about 20 percentage points. That takes the model down from 99.2 per cent accuracy to around 80 per cent. A caveat in that the epsilon I’m using as a pertubation factor, 0.3, seems high – reasonable for a first pass at the attack (I may have gotten the code wrong, after all!).</p>

<p>Here’s a particularly entertaining log:</p>

<div class="highlighter-rouge"><pre class="codehilite"><code>Test accuracy 0.992600
Test accuracy (after attack) 0.664400
Accuracy reduction 0.328200
</code></pre></div>

<p>I think an accuracy reduction of 33 percentage points is enough to make a classifier unusable. Cool!</p>

<h3 id="further-reading">Further reading</h3>
<ul>
  <li><a href="https://blog.openai.com/adversarial-example-research/">Attacking Machine Learning with Adversarial Examples</a> (OpenAI)</li>
  <li><a href="https://arxiv.org/abs/1707.05373">Houdini: Fooling Deep Structured Prediction Models</a> (arXiv)</li>
  <li><a href="https://arxiv.org/abs/1511.07528">The Limitations of Deep Learning in Adversarial Settings</a> (arXiv)</li>
  <li><a href="https://arxiv.org/abs/1412.1897">Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</a> (arXiv)</li>
  <li><a href="http://www.cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html">Is attacking machine learning easier than defending it?</a> (cleverhans)</li>
</ul>

  </div>

</article>


  <script src="/js/quotes.js"></script>
  </body>

</html>
